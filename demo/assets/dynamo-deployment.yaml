# =============================================================================
# NVIDIA Dynamo DynamoGraphDeployment - Real Example from ai-dynamo/dynamo v0.8.1
# Source: https://github.com/ai-dynamo/dynamo/blob/v0.8.1/recipes/qwen3-32b/vllm/disagg-kv-router/deploy.yaml
#
# COMPLEXITY OVERVIEW:
# - Disaggregated prefill/decode architecture (advanced ML concept)
# - Multi-service orchestration with Dynamo-specific component types
# - Requires understanding of KV-cache routing and disaggregated inference
# - RDMA/InfiniBand networking for high-performance GPU communication
# - Platform-specific (GKE, EKS, AKS) require different configurations
# =============================================================================

apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: disagg-router-6p-2d
  # PAIN POINT: namespace must have Dynamo operator watching it
  # Operator installation and namespace configuration is a prerequisite
  namespace: dynamo-system
spec:
  # ===========================================================================
  # PAIN POINT: PVCs must be pre-created (create: false)
  # Storage provisioning is a separate operational concern
  # ===========================================================================
  pvcs:
    - create: false
      name: model-cache
    - create: false
      name: compilation-cache

  services:
    # =========================================================================
    # Frontend Service - Routes requests to workers via KV-cache router
    # =========================================================================
    Frontend:
      componentType: frontend
      replicas: 1

      # PAIN POINT: HF_HOME must be consistent across all services
      # Misconfiguration causes model download failures
      envs:
        - name: HF_HOME
          value: /home/dynamo/.cache/huggingface

      resources:
        requests:
          cpu: "8"
        limits:
          cpu: "8"
      # PAIN POINT: subComponentType only applies to workers (prefill/decode)
      # Frontend doesn't use it - the official recipe includes null for clarity
      subComponentType: null

      extraPodSpec:
        mainContainer:
          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.8.1
          workingDir: /workspace
          command:
            - python
            - -m
            - dynamo.frontend
          args:
            # PAIN POINT: Users must figure out which CLI flags exist and what values to use
            # by reading source code or reverse-engineering from examples
            # PAIN POINT: Router mode affects performance characteristics
            # - "round-robin": Simple but doesn't optimize KV-cache locality
            # - "kv": Smarter routing but adds latency for routing decisions
            - --router-mode
            - kv
            # PAIN POINT: --router-reset-states purges JetStream and object store on startup
            # WARNING: Can affect existing router replicas during rolling updates
            - --router-reset-states

    # =========================================================================
    # Decode Worker - Generates output tokens (memory-bandwidth heavy)
    # =========================================================================
    VllmDecodeWorker:
      componentType: worker
      # PAIN POINT: subComponentType is Dynamo-specific
      # Must be "decode" or "prefill" for disaggregated serving
      # Wrong value silently breaks the inference pipeline
      subComponentType: decode

      # PAIN POINT: envFromSecret requires pre-created Kubernetes Secret
      # User must manually create this secret with HF_TOKEN before deploying
      envFromSecret: hf-token-secret

      replicas: 2

      resources:
        limits:
          # PAIN POINT: gpu: '2' means 2 GPUs per pod
          # Must match --tensor-parallel-size in args
          gpu: '2'
          custom:
            # PAIN POINT: RDMA/InfiniBand required for high-performance multi-GPU
            # Not all clusters have RDMA networking configured
            rdma/ib: "2"
        requests:
          gpu: '2'

      # PAIN POINT: volumeMounts must match PVC names exactly
      # Typos cause silent failures - pods start but can't cache models
      volumeMounts:
        - name: model-cache
          mountPoint: /home/dynamo/.cache/huggingface
        - name: compilation-cache
          mountPoint: /home/dynamo/.cache/vllm
          # PAIN POINT: useAsCompilationCache sets VLLM_CACHE_ROOT for persistent caching
          # Without this, compiled artifacts aren't persisted across pod restarts
          useAsCompilationCache: true

      extraPodSpec:
        mainContainer:
          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.8.1
          workingDir: /workspace
          command:
            - python3
            - -m
            - dynamo.vllm
          args:
            - --model
            # PAIN POINT: Qwen3-32B requires substantial GPU memory
            # Miscalculating memory requirements causes OOM crashes
            - Qwen/Qwen3-32B
            # PAIN POINT: tensor-parallel-size must match GPU count
            # Mismatch causes silent hangs or NCCL errors
            - --tensor-parallel-size
            - '2'
            - --disable-log-requests
            # PAIN POINT: gpu-memory-utilization at 0.90 leaves little headroom
            # Too high = OOM; too low = wasted expensive GPU memory
            - --gpu-memory-utilization
            - '0.90'
            - --no-enable-prefix-caching
            # PAIN POINT: async-scheduling is a performance optimization
            # but can cause issues with certain model architectures
            - --async-scheduling
            # PAIN POINT: block-size affects KV-cache efficiency
            # Wrong value degrades memory utilization
            - --block-size
            - '64'
            # PAIN POINT: hf-overrides requires deep knowledge of model architecture
            # This RoPE scaling config extends context from 32K to 131K tokens
            - --hf-overrides
            - '{"rope_scaling":{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768},"max_position_embeddings":131072}'
            # PAIN POINT: max-model-len of 131K tokens requires careful memory planning
            # Longer contexts = more KV-cache memory per request
            - --max-model-len
            - '131072'
          env:
            # PAIN POINT: Disabling health checks is risky in production
            # but sometimes needed for debugging startup issues
            - name: DYN_HEALTH_CHECK_ENABLED
              value: "false"
            - name: HF_HOME
              value: /home/dynamo/.cache/huggingface

    # =========================================================================
    # Prefill Worker - Processes input tokens (compute-heavy)
    # =========================================================================
    VllmPrefillWorker:
      componentType: worker
      subComponentType: prefill

      envFromSecret: hf-token-secret

      # PAIN POINT: 6 prefill workers vs 2 decode workers
      # This ratio requires understanding of your workload's prefill:decode ratio
      # Wrong ratio causes either prefill or decode bottlenecks
      replicas: 6

      resources:
        limits:
          gpu: '2'
          custom:
            rdma/ib: "2"
        requests:
          gpu: '2'

      # PAIN POINT: Prometheus annotations for metrics scraping
      # Must be configured correctly for observability to work
      extraPodMetadata:
        annotations:
          prometheus.io/scrape: "true"
          prometheus.io/port: "9400"
          prometheus.io/path: "/metrics"

      extraPodSpec:
        mainContainer:
          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.8.1
          workingDir: /workspace
          command:
            - python3
            - -m
            - dynamo.vllm
          args:
            - --model
            - Qwen/Qwen3-32B
            # PAIN POINT: --is-prefill-worker flag is critical
            # Forgetting this flag breaks disaggregated serving entirely
            - --is-prefill-worker
            # PAIN POINT: Users must figure out what all these flags mean and how to tune them
            - --tensor-parallel-size
            - '2'
            - --disable-log-requests
            - --gpu-memory-utilization
            - '0.90'
            - --async-scheduling
            - --block-size
            - '64'
            - --hf-overrides
            - '{"rope_scaling":{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768},"max_position_embeddings":131072}'
            - --max-model-len
            - '131072'
          env:
            - name: DYN_HEALTH_CHECK_ENABLED
              value: "false"
            - name: HF_HOME
              value: /home/dynamo/.cache/huggingface
