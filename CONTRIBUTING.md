# Contributing to KubeFoundry

Thank you for your interest in contributing to KubeFoundry! This guide covers development setup, project structure, and contribution guidelines.

## Development Setup

### Prerequisites

- [Bun](https://bun.sh) 1.0+
- Access to a Kubernetes cluster
- Helm CLI
- kubectl configured with cluster access

### Quick Start

```bash
# Install dependencies
bun install

# Start development servers (frontend + backend)
bun run dev

# Development mode:
#   Frontend: http://localhost:5173 (Vite dev server, proxies API to backend)
#   Backend:  http://localhost:3001
#
# Production mode (compiled binary):
#   Single server: http://localhost:3001 (frontend embedded in backend)
```

### Build Commands

```bash
# Run all tests (frontend + backend)
bun run test

# Build single binary (includes frontend)
make compile

# Lint all packages
bun run lint
```

### Individual Package Commands

**Frontend:**
```bash
bun run dev:frontend    # Start Vite dev server
bun run build:frontend  # Build for production
```

**Backend:**
```bash
bun run dev:backend     # Start with watch mode
```

## Project Structure

```
kubefoundry/
├── frontend/          # React frontend application
│   ├── src/
│   │   ├── components/  # UI components
│   │   ├── pages/       # Page components
│   │   ├── hooks/       # React hooks
│   │   └── lib/         # Utilities and API client
│   └── ...
├── backend/           # Hono backend API (runs on Bun)
│   ├── src/
│   │   ├── hono-app.ts  # All API routes consolidated
│   │   ├── index.ts     # Bun.serve() entry point
│   │   ├── providers/   # Provider implementations
│   │   │   ├── types.ts      # Provider interface
│   │   │   ├── index.ts      # Provider registry
│   │   │   ├── dynamo/       # NVIDIA Dynamo provider
│   │   │   ├── kuberay/      # KubeRay provider
│   │   │   └── kaito/        # KAITO provider (CPU/GPU via llama.cpp or vLLM)
│   │   ├── services/    # Core services
│   │   │   ├── kubernetes.ts # K8s client
│   │   │   ├── config.ts     # ConfigMap persistence
│   │   │   ├── helm.ts       # Helm CLI integration
│   │   │   ├── metrics.ts    # Prometheus metrics fetching
│   │   │   ├── autoscaler.ts # Cluster autoscaler detection
│   │   │   ├── aikit.ts      # AIKit image building
│   │   │   ├── buildkit.ts   # BuildKit builder management
│   │   │   └── registry.ts   # In-cluster registry management
│   │   ├── lib/         # Utility libraries
│   │   │   ├── k8s-errors.ts # K8s error handling
│   │   │   ├── prometheus-parser.ts # Prometheus text parser
│   │   │   └── retry.ts      # Retry logic for K8s calls
│   │   └── data/        # Static model catalog
│   └── ...
├── shared/            # Shared TypeScript types
└── docs/              # Documentation
```

## Architecture

### Provider Pattern

KubeFoundry uses a provider abstraction to support multiple inference runtimes:

```typescript
interface Provider {
  id: string;
  name: string;
  getCRDConfig(): CRDConfig;
  generateManifest(config: DeploymentConfig): object;
  parseStatus(resource: object): DeploymentStatus;
  validateConfig(config: DeploymentConfig): ValidationResult;
  checkInstallation(k8s: KubernetesService): Promise<InstallationStatus>;
  getHelmRepos(): HelmRepo[];
  getHelmCharts(): HelmChart[];
}
```

### Configuration Storage

Settings are stored in a Kubernetes ConfigMap (`kubefoundry-config`) in the `kubefoundry-system` namespace:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubefoundry-config
  namespace: kubefoundry-system
data:
  config.json: |
    {
      "defaultNamespace": "kubefoundry-system"
    }
```

**Note:** Each deployment specifies its own runtime (`provider` field). There is no global "active provider" - users select the runtime when creating a deployment.

## Environment Variables

### Frontend (.env)
```env
VITE_API_URL=http://localhost:3001
VITE_DEFAULT_NAMESPACE=kubefoundry-system
VITE_DEFAULT_HF_SECRET=hf-token-secret
```

### Backend (.env)
```env
PORT=3001
DEFAULT_NAMESPACE=kubefoundry-system
CORS_ORIGIN=http://localhost:5173
AUTH_ENABLED=false
```

## Adding a New Provider

1. **Create provider directory:**
   ```
   backend/src/providers/<name>/
   ├── index.ts    # Provider implementation
   └── schema.ts   # Zod validation schema
   ```

2. **Implement the Provider interface:**
   ```typescript
   import { Provider, CRDConfig, ... } from '../types';

   export class MyProvider implements Provider {
     id = 'my-provider';
     name = 'My Provider';
     description = '...';

     getCRDConfig(): CRDConfig { ... }
     generateManifest(config: DeploymentConfig): object { ... }
     parseStatus(resource: object): DeploymentStatus { ... }
     // ... implement all interface methods
   }
   ```

3. **Register the provider:**
   ```typescript
   // backend/src/providers/index.ts
   import { MyProvider } from './my-provider';

   providerRegistry.register(new MyProvider());
   ```

## Adding a New Model

Edit `backend/src/data/models.json`:

```json
{
  "models": [
    {
      "id": "org/model-name",
      "name": "Model Display Name",
      "description": "Brief description",
      "size": "7B",
      "task": "chat",
      "contextLength": 32768,
      "supportedEngines": ["vllm", "sglang"],
      "minGpuMemory": "16GB"
    }
  ]
}
```

## Testing API Endpoints

```bash
# Health check
curl http://localhost:3001/api/health

# Cluster status
curl http://localhost:3001/api/cluster/status

# List models
curl http://localhost:3001/api/models

# List deployments
curl http://localhost:3001/api/deployments

# Create deployment
curl -X POST http://localhost:3001/api/deployments \
  -H "Content-Type: application/json" \
  -d '{
    "name": "test-deployment",
    "namespace": "kubefoundry-system",
    "modelId": "Qwen/Qwen3-0.6B",
    "engine": "vllm",
    "mode": "aggregated",
    "replicas": 1,
    "hfTokenSecret": "hf-token-secret",
    "enforceEager": true
  }'
```

## API Endpoints

### Health & Cluster
- `GET /api/health` - Health check
- `GET /api/health/version` - Build version information
- `GET /api/cluster/status` - Kubernetes cluster status
- `GET /api/cluster/nodes` - List cluster nodes with GPU info

### Settings
- `GET /api/settings` - Get current settings and provider list
- `PUT /api/settings` - Update settings

### Runtimes
- `GET /api/runtimes/status` - Get all runtimes installation status

### Installation
- `GET /api/installation/helm/status` - Check Helm CLI availability
- `GET /api/installation/gpu-operator/status` - GPU Operator status
- `GET /api/installation/gpu-capacity` - Cluster GPU capacity
- `GET /api/installation/gpu-capacity/detailed` - Detailed GPU capacity with node pools
- `POST /api/installation/gpu-operator/install` - Install GPU Operator
- `GET /api/installation/providers/:id/status` - Get provider installation status
- `GET /api/installation/providers/:id/commands` - Get manual installation commands
- `POST /api/installation/providers/:id/install` - Install provider via Helm
- `POST /api/installation/providers/:id/upgrade` - Upgrade provider
- `POST /api/installation/providers/:id/uninstall` - Uninstall provider
- `POST /api/installation/providers/:id/uninstall-crds` - Uninstall provider CRDs

### Deployments
- `GET /api/deployments` - List all deployments
- `POST /api/deployments` - Create a new deployment
- `GET /api/deployments/:name` - Get deployment details
- `DELETE /api/deployments/:name` - Delete a deployment
- `GET /api/deployments/:name/pods` - Get deployment pods
- `GET /api/deployments/:name/logs` - Get deployment logs
- `GET /api/deployments/:name/metrics` - Get deployment metrics
- `GET /api/deployments/:name/pending-reasons` - Get pending pod reasons

### Models
- `GET /api/models` - Get model catalog
- `GET /api/models/search` - Search HuggingFace models

### Autoscaler
- `GET /api/autoscaler/detection` - Detect autoscaler type
- `GET /api/autoscaler/status` - Get autoscaler status

### AIKit (KAITO)
- `GET /api/aikit/models` - List pre-made GGUF models
- `GET /api/aikit/models/:id` - Get pre-made model details
- `POST /api/aikit/build` - Build AIKit image
- `POST /api/aikit/build/preview` - Preview image build
- `GET /api/aikit/infrastructure/status` - Check build infrastructure
- `POST /api/aikit/infrastructure/setup` - Setup build infrastructure

### OAuth & Secrets
- `GET /api/oauth/huggingface/config` - Get HuggingFace OAuth config
- `POST /api/oauth/huggingface/token` - Exchange OAuth code for token
- `GET /api/secrets/huggingface/status` - HuggingFace secret status
- `POST /api/secrets/huggingface` - Save HuggingFace token
- `DELETE /api/secrets/huggingface` - Delete HuggingFace token

## Troubleshooting

### Backend can't connect to cluster
- Verify kubectl is configured: `kubectl cluster-info`
- Check KUBECONFIG environment variable
- Ensure proper RBAC permissions

### Provider not detected as installed
- Check CRD exists: `kubectl get crd dynamographdeployments.nvidia.com`
- Check operator deployment: `kubectl get deployments -n kubefoundry`

### Frontend can't reach backend
- Check CORS_ORIGIN matches frontend URL
- Verify backend is running on correct port
- Check browser console for errors

## Code Standards

Please refer to [docs/standards.md](docs/standards.md) for coding standards and conventions.

## Pull Request Process

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Run tests (`bun run test`)
5. Run linting (`bun run lint`)
6. Commit your changes (`git commit -m 'Add amazing feature'`)
7. Push to the branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

## Questions?

Feel free to open an issue for questions or discussions about contributing.
